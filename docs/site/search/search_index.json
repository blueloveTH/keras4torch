{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Keras4Torch \"An Easy-to-Use Wrapper for Training PyTorch Models\u2764\" Keras4Torch provides an easy way to train PyTorch models in Keras style. You can use keras4torch.Model to warp any torch.nn.Module to integrate core training features. Once the model is wrapped, the training process can be done with only a few lines of code. If you are a keras user, most of your training code can work well in Keras4Torch with little change. If you are a pytorch user, Keras4Torch can help you train pytorch models with far less code than basic pytorch. Installation pip install keras4torch Keras4Torch supports PyTorch 1.6 or newer. Quick Start Let's start with a simple example of MNIST! import torch import torchvision from torch import nn import keras4torch Step1: Preprocess Data mnist = torchvision.datasets.MNIST(root='./', download=True) X, y = mnist.train_data, mnist.train_labels X = X.float() / 255.0 # scale the pixels to [0, 1] x_train, y_train = X[:40000], y[:40000] x_test, y_test = X[40000:], y[40000:] Step2: Define the Model model = torch.nn.Sequential( nn.Flatten(), nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 10) ) model = keras4torch.Model(model) # attention this line Step3: Config Optimizer, Loss and Metric model.compile(optimizer='adam', loss=nn.CrossEntropyLoss(), metrics=['acc']) Step4: Training history = model.fit(x_train, y_train, epochs=30, batch_size=512, validation_split=0.2, ) Train on 32000 samples, validate on 8000 samples: Epoch 1/30 - 0.7s - loss: 0.7440 - acc: 0.8149 - val_loss: 0.3069 - val_acc: 0.9114 - lr: 1e-03 Epoch 2/30 - 0.5s - loss: 0.2650 - acc: 0.9241 - val_loss: 0.2378 - val_acc: 0.9331 - lr: 1e-03 Epoch 3/30 - 0.5s - loss: 0.1946 - acc: 0.9435 - val_loss: 0.1940 - val_acc: 0.9431 - lr: 1e-03 Epoch 4/30 - 0.5s - loss: 0.1513 - acc: 0.9555 - val_loss: 0.1663 - val_acc: 0.9524 - lr: 1e-03 ... ... Step5: Plot Learning Curve history.plot(kind='line', y=['acc', 'val_acc']) Step6: Evaluate on Test Set model.evaluate(x_test, y_test) {'loss': 0.121063925, 'acc': 0.9736} Communication If you have any problem when using Keras4Torch, please: open a Github Issue send email to blueloveTH@foxmail.com or zhangzhipengcs@foxmail.com. Keras4Torch is still under development. Any contribution to us would be more than welcome : ) You can contribute new features by opening a Pull Request. (The details will be updated soon)","title":"Home"},{"location":"#keras4torch","text":"\"An Easy-to-Use Wrapper for Training PyTorch Models\u2764\" Keras4Torch provides an easy way to train PyTorch models in Keras style. You can use keras4torch.Model to warp any torch.nn.Module to integrate core training features. Once the model is wrapped, the training process can be done with only a few lines of code. If you are a keras user, most of your training code can work well in Keras4Torch with little change. If you are a pytorch user, Keras4Torch can help you train pytorch models with far less code than basic pytorch.","title":"Keras4Torch"},{"location":"#installation","text":"pip install keras4torch Keras4Torch supports PyTorch 1.6 or newer.","title":"Installation"},{"location":"#quick-start","text":"Let's start with a simple example of MNIST! import torch import torchvision from torch import nn import keras4torch","title":"Quick Start"},{"location":"#step1-preprocess-data","text":"mnist = torchvision.datasets.MNIST(root='./', download=True) X, y = mnist.train_data, mnist.train_labels X = X.float() / 255.0 # scale the pixels to [0, 1] x_train, y_train = X[:40000], y[:40000] x_test, y_test = X[40000:], y[40000:]","title":"Step1: Preprocess Data"},{"location":"#step2-define-the-model","text":"model = torch.nn.Sequential( nn.Flatten(), nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 128), nn.ReLU(), nn.Linear(128, 10) ) model = keras4torch.Model(model) # attention this line","title":"Step2: Define the Model"},{"location":"#step3-config-optimizer-loss-and-metric","text":"model.compile(optimizer='adam', loss=nn.CrossEntropyLoss(), metrics=['acc'])","title":"Step3: Config Optimizer, Loss and Metric"},{"location":"#step4-training","text":"history = model.fit(x_train, y_train, epochs=30, batch_size=512, validation_split=0.2, ) Train on 32000 samples, validate on 8000 samples: Epoch 1/30 - 0.7s - loss: 0.7440 - acc: 0.8149 - val_loss: 0.3069 - val_acc: 0.9114 - lr: 1e-03 Epoch 2/30 - 0.5s - loss: 0.2650 - acc: 0.9241 - val_loss: 0.2378 - val_acc: 0.9331 - lr: 1e-03 Epoch 3/30 - 0.5s - loss: 0.1946 - acc: 0.9435 - val_loss: 0.1940 - val_acc: 0.9431 - lr: 1e-03 Epoch 4/30 - 0.5s - loss: 0.1513 - acc: 0.9555 - val_loss: 0.1663 - val_acc: 0.9524 - lr: 1e-03 ... ...","title":"Step4: Training"},{"location":"#step5-plot-learning-curve","text":"history.plot(kind='line', y=['acc', 'val_acc'])","title":"Step5: Plot Learning Curve"},{"location":"#step6-evaluate-on-test-set","text":"model.evaluate(x_test, y_test) {'loss': 0.121063925, 'acc': 0.9736}","title":"Step6: Evaluate on Test Set"},{"location":"#communication","text":"If you have any problem when using Keras4Torch, please: open a Github Issue send email to blueloveTH@foxmail.com or zhangzhipengcs@foxmail.com. Keras4Torch is still under development. Any contribution to us would be more than welcome : ) You can contribute new features by opening a Pull Request. (The details will be updated soon)","title":"Communication"},{"location":"api_references/callbacks_api/","text":"","title":"Callbacks API"},{"location":"api_references/layers_api/","text":"Layers API keras4torch.layers provides KerasLayer for automatic shape inference as well as some other useful layers for quick experiment. Built-in KerasLayer The built-in KerasLayer is a replacement for some native torch module. The followings are supported. Conv1d Conv2d Conv3d Linear GRU LSTM BatchNorm1d BatchNorm2d BatchNorm3d Compared with native torch modules, what you need to change is omitting the first shape parameter. For example, nn.Linear(128, 512) must be rewritten as keras4torch.layers.Linear(512) ; nn.Conv1d(32, 64, kernel_size=3) must be rewritten as keras4torch.layers.Conv1d(64, kernel_size=3) . If a model contains KerasLayer , you should build it after wrapping it by keras4torch.Model . model = k4t.Model(model) # the model contains at least one `KerasLayer` model.build(input_shape=[128]) The argument input_shape should not include batch_size dimension. Custom KerasLayer In fact, what a KerasLayer do is to delay the instantiation to the first module.forward() call, thus it can get the output shape from its previous layer and decide how many channels should be created. To write your own KerasLayer for automatic shape inference, you need to subclass KerasLayer and implement its abstract method build() , as the source code shown below. class KerasLayer(nn.Module): def __init__(self, *args, **kwargs) -> None: super(KerasLayer, self).__init__() self.args = args self.kwargs = kwargs self.module = None @abstractclassmethod def build(self, in_shape: torch.Size): pass def forward(self, x): if not self.module: self.module = self.build(x.shape) return self.module.forward(x) In KerasLayer.build() , you will get the current input shape to instantiate the actual module and return it. The arguments are stored by self.args and self.kwargs . class Conv1d(KerasLayer): def build(self, in_shape): return nn.Conv1d(in_shape[1], *self.args, **self.kwargs) Note that KerasLayer should be used as a wrapper only, which means your native torch module need to be available without KerasLayer . Never write logics in KerasLayer irrelevant with in_shape . Others Lambda Add Reshape Concatenate SamePadding","title":"Layers API"},{"location":"api_references/layers_api/#layers-api","text":"keras4torch.layers provides KerasLayer for automatic shape inference as well as some other useful layers for quick experiment.","title":"Layers API"},{"location":"api_references/layers_api/#built-in-keraslayer","text":"The built-in KerasLayer is a replacement for some native torch module. The followings are supported. Conv1d Conv2d Conv3d Linear GRU LSTM BatchNorm1d BatchNorm2d BatchNorm3d Compared with native torch modules, what you need to change is omitting the first shape parameter. For example, nn.Linear(128, 512) must be rewritten as keras4torch.layers.Linear(512) ; nn.Conv1d(32, 64, kernel_size=3) must be rewritten as keras4torch.layers.Conv1d(64, kernel_size=3) . If a model contains KerasLayer , you should build it after wrapping it by keras4torch.Model . model = k4t.Model(model) # the model contains at least one `KerasLayer` model.build(input_shape=[128]) The argument input_shape should not include batch_size dimension.","title":"Built-in KerasLayer"},{"location":"api_references/layers_api/#custom-keraslayer","text":"In fact, what a KerasLayer do is to delay the instantiation to the first module.forward() call, thus it can get the output shape from its previous layer and decide how many channels should be created. To write your own KerasLayer for automatic shape inference, you need to subclass KerasLayer and implement its abstract method build() , as the source code shown below. class KerasLayer(nn.Module): def __init__(self, *args, **kwargs) -> None: super(KerasLayer, self).__init__() self.args = args self.kwargs = kwargs self.module = None @abstractclassmethod def build(self, in_shape: torch.Size): pass def forward(self, x): if not self.module: self.module = self.build(x.shape) return self.module.forward(x) In KerasLayer.build() , you will get the current input shape to instantiate the actual module and return it. The arguments are stored by self.args and self.kwargs . class Conv1d(KerasLayer): def build(self, in_shape): return nn.Conv1d(in_shape[1], *self.args, **self.kwargs) Note that KerasLayer should be used as a wrapper only, which means your native torch module need to be available without KerasLayer . Never write logics in KerasLayer irrelevant with in_shape .","title":"Custom KerasLayer"},{"location":"api_references/layers_api/#others","text":"Lambda Add Reshape Concatenate SamePadding","title":"Others"},{"location":"api_references/losses/","text":"","title":"Losses"},{"location":"api_references/metrics/","text":"","title":"Metrics"},{"location":"api_references/models_api/","text":"Models API There are two ways to instantiate a keras4torch.Model . 1 - Wrap a PyTorch Module keras4torch.Model wraps a torch.nn.Module to integrate training and inference features. Training Pipeline compile fit fit_dl evaluate predict Saving & Serialization save_weights load_weights Utilities summary count_params 2 - Use Functional API (Beta) Functional API allows you to build layers by functional programming. Step1: Create a functional object import keras4torch as k4t fn = k4t.models.Functional() Step2: Define your input inputs = fn.input([64]) fn.input(input_shape, dtype) returns a symbolic tensor which has a shape of [batch_size, 64]. Step3: Call the layers seq = fn.call(k4t.layers.Linear(64), inputs) seq = fn.call(k4t.layers.Add(), [seq, inputs]) output = fn.call(nn.Softmax(-1), seq) This code block generates a residual connection between the original input and the output of the Linear layer, and defines a softmax activation as the final output. Step4: Build the model model = fn.build(output) This line will build the functional object and convert it to keras4torch.Model . You can use it for training at once.","title":"Models API"},{"location":"api_references/models_api/#models-api","text":"There are two ways to instantiate a keras4torch.Model .","title":"Models API"},{"location":"api_references/models_api/#1-wrap-a-pytorch-module","text":"keras4torch.Model wraps a torch.nn.Module to integrate training and inference features.","title":"1 - Wrap a PyTorch Module"},{"location":"api_references/models_api/#training-pipeline","text":"compile fit fit_dl evaluate predict","title":"Training Pipeline"},{"location":"api_references/models_api/#saving-serialization","text":"save_weights load_weights","title":"Saving &amp; Serialization"},{"location":"api_references/models_api/#utilities","text":"summary count_params","title":"Utilities"},{"location":"api_references/models_api/#2-use-functional-api-beta","text":"Functional API allows you to build layers by functional programming.","title":"2 - Use Functional API (Beta)"},{"location":"api_references/models_api/#step1-create-a-functional-object","text":"import keras4torch as k4t fn = k4t.models.Functional()","title":"Step1: Create a functional object"},{"location":"api_references/models_api/#step2-define-your-input","text":"inputs = fn.input([64]) fn.input(input_shape, dtype) returns a symbolic tensor which has a shape of [batch_size, 64].","title":"Step2: Define your input"},{"location":"api_references/models_api/#step3-call-the-layers","text":"seq = fn.call(k4t.layers.Linear(64), inputs) seq = fn.call(k4t.layers.Add(), [seq, inputs]) output = fn.call(nn.Softmax(-1), seq) This code block generates a residual connection between the original input and the output of the Linear layer, and defines a softmax activation as the final output.","title":"Step3: Call the layers"},{"location":"api_references/models_api/#step4-build-the-model","text":"model = fn.build(output) This line will build the functional object and convert it to keras4torch.Model . You can use it for training at once.","title":"Step4: Build the model"},{"location":"api_references/utilities/","text":"","title":"Utilities"},{"location":"code_examples/001/","text":"","title":"001"},{"location":"code_examples/002/","text":"","title":"002"}]}